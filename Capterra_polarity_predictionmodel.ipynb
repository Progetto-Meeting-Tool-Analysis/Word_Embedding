{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\illa\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\illa\\anaconda3\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\illa\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\illa\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import re\n",
    "import itertools  \n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01 = pd.read_csv('df_settore_vantaggi_inconvenienti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_01['vantaggi/inconvenienti'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_0 = df_01['vantaggi/inconvenienti']\n",
    "y = df_01['bin_y'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hangout', 'meet', 'is', 'already', 'available', 'from', 'within', 'gmail', 'it', 'simple', 'to', 'add', 'to', 'a', 'meeting', 'invitation', 'from', 'gcal', 'or', 'to', 'launch', 'for', 'impromptu', 'meeting', 'from', 'the', 'gsuite', 'launchpad']\n",
      "['ive', 'tried', 'other', 'solution', 'and', 'for', 'me', 'the', 'best', 'from', 'this', 'solution', 'is', 'how', 'easy', 'is', 'to', 'access', 'to', 'the', 'conference', 'for', 'my', 'customer', 'just', 'one', 'click', 'and', 'they', 'are', 'inside', 'the', 'conference']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#regex rimuoviamo la punteggiatura\n",
    "text = text_0.str.lower().str.replace('[^\\w\\s]','')\n",
    "#seperiamo il testo con split\n",
    "text = text.str.split()\n",
    "# text = text.apply(lambda x: [lemmatizer.lemmatize(word) for sentence in x for word in sentence])\n",
    "text = text.apply(lambda x: [lemmatizer.lemmatize(sent) for sent in x])\n",
    "print(text[5])\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hangout', 'meet', 'is', 'already', 'available', 'from', 'within', 'gmail', 'it', 'simple', 'to', 'add', 'to', 'a', 'meeting', 'invitation', 'from', 'gcal', 'or', 'to', 'launch', 'for', 'impromptu', 'meeting', 'from', 'the', 'gsuite', 'launchpad']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = pd.read_csv('stop_list.txt', sep=\" \", header=None)\n",
    "stop.extend(['good', 'many', 'love', 'excellent', 'would'])\n",
    "\n",
    "#min_count=entrambe le parole devono comparire almeno 5volte\n",
    "#thresold=frequenza quante volte compaiono una dopo l'altra\n",
    "#common_terms=vengono inseriti in mezzo se presenti, mai all'inizio o fine\n",
    "bigram = Phrases(text, min_count=5, threshold=0.3, common_terms=stop)\n",
    "print(bigram[text[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ive', 'tried', 'other', 'solution', 'and', 'for', 'me', 'the', 'best', 'from', 'this', 'solution', 'is', 'how', 'easy', 'is', 'to', 'access', 'to', 'the', 'conference', 'for', 'my', 'customer', 'just', 'one', 'click', 'and', 'they', 'are', 'inside', 'the', 'conference']\n"
     ]
    }
   ],
   "source": [
    "print(bigram[text[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ive', 'tried', 'other', 'solution', 'and', 'for', 'me', 'the', 'best', 'from', 'this', 'solution', 'is', 'how', 'easy', 'is', 'to', 'access', 'to', 'the', 'conference', 'for', 'my', 'customer', 'just', 'one', 'click', 'and', 'they', 'are', 'inside', 'the', 'conference']\n"
     ]
    }
   ],
   "source": [
    "bigrams = [bigram[item] for item in text]\n",
    "ngrams = [bigram[item] for item in bigrams] #4gram bigram del bigram\n",
    "print(ngrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ive', 'tried', 'other', 'solution', 'and', 'for', 'me', 'the', 'best', 'from', 'this', 'solution', 'is', 'how', 'easy', 'is', 'to', 'access', 'to', 'the', 'conference', 'for', 'my', 'customer', 'just', 'one', 'click', 'and', 'they', 'are', 'inside', 'the', 'conference']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tried solution solution access conference customer click inside conference'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = pd.read_csv('stop_list.txt', sep=\" \", header=None)\n",
    "stop = stop_words[0].tolist()\n",
    "stop.extend(['good', 'bad', 'dont', 'many', 'love', 'excellent', 'would', 'perfect', 'even', 'great'])\n",
    "print(ngrams[0])\n",
    "train_sentences = []\n",
    "for row in ngrams:\n",
    "    train_sentences.append(' '.join([item for item in row if item not in stop]))\n",
    "# train_sentences = [' '.join(item) for item in ngrams]\n",
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01['vantaggi/svantaggi'] = train_sentences\n",
    "df_01.to_csv('df_settore_vantaggi_svantaggi_binary.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#limitare le features per overfitting\n",
    "#vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=1000)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=1000)\n",
    "X = vectorizer.fit_transform(train_sentences)\n",
    "#riottengo il nome delle features che prima sono \n",
    "#state converite in chiavi numeriche\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "#X = X.toarray()\n",
    "#le porti in nparray\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(<39436x1000 sparse matrix of type '<class 'numpy.float64'>'\n\twith 265458 stored elements in Compressed Sparse Row format>,\n      dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-b2ec82dab5d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#il dataset è sbilanciato, riporto con stratify la stessa proporzione\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#sia nel train che test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2116\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2118\u001b[1;33m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \"\"\"\n\u001b[0;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \"\"\"\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[1;32m--> 152\u001b[1;33m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[0;32m    153\u001b[0m         \u001b[1;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m# Dask dataframes may not return numeric shape[0] value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array array(<39436x1000 sparse matrix of type '<class 'numpy.float64'>'\n\twith 265458 stored elements in Compressed Sparse Row format>,\n      dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#il dataset è sbilanciato, riporto con stratify la stessa proporzione \n",
    "#sia nel train che test\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "stop_words = pd.read_csv('stop_list.txt', sep=\" \", header=None)\n",
    "stop = stop_words[0].tolist()\n",
    "stop.extend(['good', 'bad', 'dont'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "x = x_0.str.lower().str.replace('[^\\w\\s]','')\n",
    "x = x.str.split()\n",
    "x = x.apply(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "x = x.apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ive_tried', 'other', 'solution', 'and', 'for', 'me', 'the', 'best', 'from', 'this', 'solution', 'is', 'how', 'easy', 'is', 'to', 'access', 'to', 'the', 'conference', 'for', 'my', 'customer', 'just', 'one_click', 'and', 'they', 'are', 'inside', 'the', 'conference']\n"
     ]
    }
   ],
   "source": [
    "bigrams = [bigram[item] for item in text]\n",
    "ngrams = [bigram[item] for item in bigrams] #4gram bigram del bigram\n",
    "print(ngrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#impostiamo lunghezza mx review\n",
    "max_length = 20\n",
    "#ogni parola del dizionario viene identificata da un numero\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(x)\n",
    "#size + 1 perché len conta a partire da 0\n",
    "vocab_size = len(t.word_index) + 1\n",
    "#conto i vocaboli risultanti dal modello di embedding \n",
    "emb_size=embeddings.vector_size\n",
    "#trasformo parole in numeri\n",
    "encoded_docs = t.texts_to_sequences(x)\n",
    "#se la frase è maggiore di 20parole la tronco, se inferiore aggiungo 0 fino a 20\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_docs, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0, in questo caso 20 zeri\n",
    "    weight_matrix = np.zeros((vocab_size, emb_size))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        if word in embeddings.vocab:\n",
    "            weight_matrix[i] = embedding[word]\n",
    "    return weight_matrix\n",
    "#risostituisco a ciascun valore del token una lista di 20 embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13985, 100)\n",
      "***CROSS VALIDATED GRID SEARCH***\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, LSTM, Bidirectional, GRU, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(embeddings, t.word_index)\n",
    "print(embedding_vectors.shape)\n",
    "e = Embedding(vocab_size, emb_size, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "\n",
    "#params\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "n_labels = 2\n",
    "\n",
    "# create the model\n",
    "def baseline_model(optimizer=opt):\n",
    "    model = Sequential()\n",
    "    model.add(e)\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    model.add(Dense(n_labels, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=baseline_model)\n",
    "batch_size = [32, 64, 128] #quanta porzione prendo del corpus\n",
    "epochs = [5, 10, 20]\n",
    "optimizer = ['Adadelta', 'Nadam']\n",
    "#batch_size = [64]\n",
    "#epochs = [5]\n",
    "#optimizer = ['Adam']\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='f1_weighted', verbose=2)\n",
    "print('***CROSS VALIDATED GRID SEARCH***')\n",
    "%time grid_result = grid.fit(x_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\illa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\wrappers\\scikit_learn.py:241: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "***RESULTS ON TEST SET***\n",
      "accuracy_score 0.884600465477114\n",
      "f1_score 0.9385838150289019\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.03      0.05      1135\n",
      "           1       0.89      0.99      0.94      9177\n",
      "\n",
      "    accuracy                           0.88     10312\n",
      "   macro avg       0.57      0.51      0.49     10312\n",
      "weighted avg       0.82      0.88      0.84     10312\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "#results on the test set for the best model\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('***RESULTS ON TEST SET***')\n",
    "print(\"accuracy_score\", accuracy)\n",
    "print(\"f1_score\", f1)\n",
    "print('\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
